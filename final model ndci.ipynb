{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "from odc.stac import configure_rio, stac_load\n",
    "import dask.distributed\n",
    "import dask.utils\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Dask client for parallel processing\n",
    "client = dask.distributed.Client()\n",
    "configure_rio(cloud_defaults=True, client=client)\n",
    "\n",
    "# Configure rio with dynamic resolution\n",
    "resolution = 20\n",
    "memory_limit = dask.utils.parse_bytes(client.cluster.workers[0].memory_manager.memory_limit)\n",
    "SHRINK = 4\n",
    "if memory_limit < dask.utils.parse_bytes(\"4G\"):\n",
    "    SHRINK = 8  # Adjust chunk size if memory is limited\n",
    "\n",
    "resolution = resolution * SHRINK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the area of interest (AOI) for Lake Michigan\n",
    "area_of_interest = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            [-88.2, 43.0],  # Lower-left corner\n",
    "            [-86.1, 43.0],  # Lower-right corner\n",
    "            [-86.1, 45.0],  # Upper-right corner\n",
    "            [-88.2, 45.0],  # Upper-left corner\n",
    "            [-88.2, 43.0],  # Closing the polygon\n",
    "        ]\n",
    "    ],\n",
    "}\n",
    "\n",
    "#  time span of 3 months\n",
    "time_of_interest = \"2023-06-01/2023-12-01\"\n",
    "\n",
    "# Query the catalog for the data\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    intersects=area_of_interest,\n",
    "    datetime=time_of_interest\n",
    ")\n",
    "items = list(search.items())\n",
    "print(f\"Returned {len(items)} Items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the catalog with dynamic chunking and handle missing data\n",
    "xx = stac_load(\n",
    "    items,\n",
    "    chunks={\"x\": 1024 * SHRINK, \"y\": 1024 * SHRINK},  # Dynamically adjust chunk size\n",
    "    patch_url=planetary_computer.sign,\n",
    "    resolution=resolution,\n",
    "    dtype=\"uint16\",  # Handle missing data by marking nodata values\n",
    "    nodata=0\n",
    ")\n",
    "\n",
    "# Display loaded data\n",
    "print(f\"Bands: {','.join(list(xx.data_vars))}\")\n",
    "display(xx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert data to float and handle missing nodata values\n",
    "def to_float(xx, nodata_value=None):\n",
    "    _xx = xx.astype(\"float32\")  # Convert data to float32 for precision\n",
    "    if nodata_value is None:\n",
    "        nodata_value = _xx.attrs.pop(\"nodata\", None)  # Fetch nodata value if exists\n",
    "    if nodata_value is not None:\n",
    "        return _xx.where(xx != nodata_value)  # Replace nodata with NaN\n",
    "    return _xx\n",
    "\n",
    "# Convert specific bands to float32 and handle missing data\n",
    "b05 = to_float(xx.B05)  # Red-Edge band\n",
    "b04 = to_float(xx.B04)  # Red band\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NDCI with small constant to avoid division by zero\n",
    "ndci = (b05 - b04) / (b05 + b04 + 1e-6)\n",
    "\n",
    "# Display the calculated NDCI\n",
    "display(ndci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndci = ndci.fillna(ndci.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chl = 826.57*(ndci**3) - 176.43*(ndci**2) + 19*(ndci) + 4.071\n",
    "display(chl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of coordinates (first 50 coordinates)\n",
    "chl_subset = chl[:, :50, :50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chl_subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare the time series data for LSTM\n",
    "def create_dataset(data, time_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])\n",
    "        y.append(data[i + time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set time steps (e.g., using last 5 values to predict the next)\n",
    "time_steps = 10\n",
    "#forecast_horizon = 5  # How many future timesteps to predict\n",
    "X, y = create_dataset(chl_subset, time_steps)\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input to be [samples, time steps, features]\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], chl_subset.shape[1] * chl_subset.shape[2]))\n",
    "y = np.reshape(y, (y.shape[0], chl_subset.shape[1] * chl_subset.shape[2]))\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Normalize input features\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "\n",
    "X_test_scaled = scaler_X.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# Normalize target variable\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Reshape back to the original shape\n",
    "y_train_scaled = y_train_scaled.reshape(y_train.shape)  \n",
    "y_test_scaled = y_test_scaled.reshape(y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_scaled.shape, y_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(time_steps, X.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(y.shape[1], activation='linear')\n",
    "])\n",
    "# Output a single value for each spatial location (the predicted next time point)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train_scaled, epochs=20, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = model.evaluate(X_test_scaled, y_test_scaled)\n",
    "print(f\"Model loss (MSE): {results}\")\n",
    "print(\"RMSE: \", np.sqrt(results))\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_scaled)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_scaled.min(), y_test_scaled.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.min(), predictions.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = scaler_y.inverse_transform(predictions.reshape(-1, 1))\n",
    "\n",
    "# Reshape the predictions back to the original shape\n",
    "y_pred = y_pred.reshape(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred.min(), y_pred.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Save predictions as a Numpy .npy file\n",
    "# np.save(\"predictions.npy\", y_pred)\n",
    "# np.save(\"y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape y_test_inverse back to its original shape\n",
    "y_test_inverse = y_test.reshape((y_test.shape[0], chl_subset.shape[1], chl_subset.shape[2]))\n",
    "print(y_test_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_inverse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape y_test_inverse back to its original shape\n",
    "y_pred_inverse = y_pred.reshape((y_pred.shape[0], chl_subset.shape[1], chl_subset.shape[2]))\n",
    "print(y_pred_inverse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a timestep to visualize (e.g., the first timestep in the test set)\n",
    "timestep = 0\n",
    "\n",
    "# Extract the data for the selected timestep\n",
    "y_pred_timestep = y_pred_inverse[timestep]\n",
    "\n",
    "# Extract the actual coordinates\n",
    "x_coords = chl_subset.coords['x'].values\n",
    "y_coords = chl_subset.coords['y'].values\n",
    "\n",
    "# Create the visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pcolormesh(x_coords, y_coords, y_pred_timestep, cmap='viridis', shading='auto')\n",
    "plt.colorbar(label='Chlorophyll Value')\n",
    "plt.title(f'Predicted Chlorophyll Values at Timestep {timestep}')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a specific grid point (e.g., x_idx = 0, y_idx = 0 for top-left corner)\n",
    "x_idx, y_idx = 0, 0\n",
    "\n",
    "# Extract chlorophyll values over time for the specific point\n",
    "time_series = y_pred_inverse[:, y_idx, x_idx]\n",
    "\n",
    "# Plot chlorophyll values over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(time_series)), time_series, marker='o', linestyle='-', label=f'Point ({x_idx}, {y_idx})')\n",
    "plt.title('Chlorophyll Value Over Time at Specific Grid Point')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Chlorophyll Value')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM_model.h5')\n",
    "print(\"Model saved to LSTM_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the current_input array to a .npy file\n",
    "np.save('test_input.npy', X_test_scaled )\n",
    "print(\"current_input saved to test_input.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the y_train array to a .npy file\n",
    "np.save('y_train.npy', y_train)\n",
    "print(\"y_train saved to y_train.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
